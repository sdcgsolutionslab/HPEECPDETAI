{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a5e4bcf-e5b6-40a3-9135-b8d583505348",
   "metadata": {},
   "source": [
    "# Getting started with Determined, the open-source deep learning training platform - Lab 4\n",
    "## Hyperparameter Optimization with Determined\n",
    "\n",
    "Next, let's run an experiment with the same model code, but this time leveraging Determined's hyperparameter tuning (aka Hyperparameter Optimization or **HPO**) to improve the model accuracy and let you find the best combination of hyperparameters for your particular model. \n",
    "\n",
    "ML engineers typically use HPO to efficiently determine the hyperparameter values that yield the best-performing model. Here in this lab, the hyperparameters in the experiment configuration file are specified as ranges rather than fixed values, and the `Adaptive ASHA` searcher method is used to explore the hyperparameter space helping you find the best hyperparameters for your model.\n",
    "\n",
    "**With HPO, an experiment consists of multiple training tasks (trials)** running simultaneously on different GPUs. Each of the trials trains the model on the same dataset and code for the DL model. However each trial uses a different configuration of hyperparameters randomly chosen from the range of values defined in the experiment configuration file.\n",
    "\n",
    "For this part of lab, the number of trials to run, the set of user-defined hyperparameters range, the searcher method, and the amount of data (batches or epochs) on which to train the model are defined in the experiment configuration file _adaptive.yaml_.\n",
    "\n",
    ">Note: The _Adaptive ASHA_ searcher is a state-of-the-art method that is used to find effective hyperparameter settings within a predifined range of hyperparameter values.\n",
    "\n",
    "***More about Hyperparameter optimization and Searcher methods supported by Determined can be found [here](https://docs.determined.ai/latest/training-hyperparameter/index.html#hyperparameter-tuning)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab41b4a-d8b2-4b09-a064-cec710e91ef6",
   "metadata": {},
   "source": [
    "### 1- Create an experiment to train multiple models as part of a hyperparameter search, using Determined hyperparameter optimization (HPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f696291e-bcba-48a1-9ca6-ec1f88a9846c",
   "metadata": {},
   "source": [
    "Let's run an experiment with the same model definition (same code), but this time leveraging Determined's hyperparameter optimization functionality using the _adaptive.yaml_ experiment configuration file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92247a4-4e2e-416f-ba7d-4fe5f568b9c2",
   "metadata": {},
   "source": [
    "#### First, let's take a closer look at the experiment configuration file for HPO: adaptive.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f45091-4fb7-44a1-8529-18280f25cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/source_control/Code/adaptive.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b4423-1fe0-4c20-9925-8007455e8033",
   "metadata": {},
   "source": [
    "As you can see here, the experiment configuration file _adaptive.yaml_ tells Determined which searcher algorithm to use (adaptive_asha) and the ranges to explore for each hyperparameter. In the searcher section, the parameter ***max_trials*** indicates the total number of trials that the experiment will create and how many model configuration to explore. Each trial run on one GPU because the resource parameter _slot_per_trial_ is not specified, therefore the default setting of _slot_per_trial=1_ is used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accc85db-ffbf-4e5b-8dd8-a727c53c2abd",
   "metadata": {},
   "source": [
    "#### Next, submit the experiment with the experiment configuration file _adaptive.yaml_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574e8b41-60aa-44c3-a69f-445d413849b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "masterUrl=!kubectl describe service determined-master-service-stagingdetai -n determinedai | grep gateway/8080 | awk '{print $3}'\n",
    "det_master = str(masterUrl)[2:-2] # we remove any potential brackets\n",
    "determined_master = \"http://\" + det_master\n",
    "#\n",
    "# Launch experiment to train the model with hyperparameter tuning\n",
    "!~/.local/bin/det -m {determined_master} experiment create ~/source_control/Code/adaptive.yaml ~/source_control/Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5840e24-bc40-4f57-8ba9-0c978b1b78bb",
   "metadata": {},
   "source": [
    "In the lab environment, each Kubernetes worker host has one GPU only. Therefore, each training task (trial) in the experiment will run on one GPU. \n",
    "\n",
    ">**Note:** In an environment with many multi-GPU devices, you could combine HPO and Distributing Training and assign more than one GPU to each trial in the experiment by defining the parameter _slot_per_trial_ in the experiment configuration file much like the Distributed Training you examined earlier.\n",
    "\n",
    "#### Using the command below, you will see that Determined Master has schedules multiple trials for your experiment in the Kubernetes cluster, each of which will use it own GPU. The POD name (one per trial) for your experiment is in the form:\n",
    "\n",
    " _exp-\\<experimentID\\>-trial-\\<TriaID\\>-\\<unique-name\\>_\n",
    "\n",
    "> Notice the trial PODs have been assigned a different trial ID for your experiment, which means your experiment features multiple trials each with a different set of hyperparameters. \n",
    "\n",
    "> <font color=\"blue\"> **Note:** As you are sharing the same Kubernetes resources with other participants, and depending on the number of concurrent experiments running, your training tasks PODs might be in **Pending** state waiting for GPU resources to become available. You might need to wait a few minutes until other experiments complete for your training tasks PODs to become **Running**.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610cf4e2-b408-4e2d-93b0-6658fc9a0e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods -n determinedai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d543ff43-cd23-4f57-af3b-76fc8af6cd28",
   "metadata": {},
   "source": [
    "#### Run the code cell below to monitor the execution progress of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39cfafa-83db-4658-8f5b-3c448c6f8962",
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/.local/bin/det -m {determined_master} experiment list | tail -1\n",
    "# Get the experiment Id, remove spaces\n",
    "myexpId=!~/.local/bin/det -m {determined_master} experiment list | tail -1 | cut -d'|' -f 1 |  tr -d ' '\n",
    "# remove the trailer characters\n",
    "myexpId=str(myexpId)[2:-2]\n",
    "!~/.local/bin/det -m {determined_master} experiment describe {myexpId} --json | jq .[0].state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9d0800-e1cb-4eba-9ec9-6157b559edc6",
   "metadata": {},
   "source": [
    "### 2- Monitor and visualize your experiment in Determined Web User Interface\n",
    "\n",
    "Determined will run the number of _max_trials_ trials and automatically start new trials as resources become available.\n",
    "\n",
    "To monitor the progress of the training task and access information on both training and validation performance for the trials of your experiment, you can simply return to the Determined **WebUI**.\n",
    "\n",
    "##### From the **Dashboard**, after a minute or so, you should see the experiment as an **active** state and the completion percentage. \n",
    "\n",
    "> <font color=\"blue\"> **Important Note:** If there are multiple concurrent participants to the workshop, your experiment might not run yet because there are more experiments running than the Kubernetes cluster has GPUs. You might need to wait a few minutes until other experiments complete for your experiment to start running. </font>\n",
    "\n",
    "##### Select your most recent experiment.\n",
    "\n",
    "As you can see in the **Visualization** pane, Determinedâ€™s hyperparameter optimization provides you with several visualization options for analyzing results: Learning Curve, Parallel Plot, Scatter Plot, Heatmap.\n",
    "\n",
    ">***Note:*** To learn more about these visualization options, please check out the blog post [here](https://www.determined.ai/blog/hyperparameter-visualizations-determined).\n",
    "\n",
    "<img src=\"WebUI-Exp-adaptive-visualization.png\" height=\"154\" width=\"900\">\n",
    "\n",
    "\n",
    "As the experiment runs, the _Learning Curve_ graph is showing the model validation accuracy metric (_val_categorical_accuracy_). From the **Metric** drop-down list, under **Training Metrics**, select _categorical_accuracy_. Click the ***Apply*** button as shown in the picture above to visualize the model accuracy on training data for each trial over the number of completed batches. \n",
    "\n",
    "<img src=\"WebUI-Exp-adaptive-graphs.png\" height=\"394\" width=\"900\">\n",
    "\n",
    "After the experiment completes, you might see that Determined's hyperparameter Searcher Adaptive ASHA's ***early stopping*** capability has stopped poor performing trials that do not require any extra training. Determined releases valuable GPU resources on trials that will never produce the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a7c388-54a6-4f8f-b3a1-73b605948c7e",
   "metadata": {},
   "source": [
    "### 3 - Get the trial and hyperparameters that yields to the best model\n",
    "\n",
    "Like the other experiments you explored earlier, you can use the command below to list the trial that yields to the best model:\n",
    "\n",
    "* _det experiment list-checkpoints [--best] [N best checkpoints to return] \\<experiment_Id\\>_\n",
    "\n",
    "#### Run the code cell below to display the trial that yields to the best model for your experiment\n",
    "\n",
    "You can see on the experiment detail page that training the model with the hyperparameter settings in `adaptive.yaml` yields a validation accuracy between 93% and 97%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59096343-04b4-47a5-aab4-30244e8b1cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list the best Trial checkpoint(s) (training task):\n",
    "!~/.local/bin/det -m {determined_master} experiment list-checkpoints --best 1 {myexpId}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fd051f-871c-4704-a2be-3ea72aac1d80",
   "metadata": {},
   "source": [
    "You can use the command below to discover the hyperparameters that yield to the best model:\n",
    "\n",
    "* _det trial describe \\<trial_Id\\>_\n",
    "\n",
    "#### Run the code cell below to display the hyperparameters that yield to the best model for your experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4461a5-c127-4cd7-b358-c49f00e27f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestTrialId=!~/.local/bin/det -m {determined_master} experiment list-checkpoints --best 1 {myexpId} | head -3 | tail -1 | cut -d'|' -f 1 |  tr -d ' '\n",
    "bestTrialId=str(bestTrialId)[2:-2]\n",
    "!~/.local/bin/det -m {determined_master} trial describe {bestTrialId}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52f6c59-7da5-4fd3-8b9e-fdd15e3926f7",
   "metadata": {},
   "source": [
    ">Note: Unlike the other experiment you explored earlier, the _adaptive.yaml_ experiment configuration file does not define a periodic validation parameter (min_validation_period). Therefore the validated model is checkpointed at the trial end. Only one checkpoint is saved for each trial. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f5d7d0-e7ed-4706-92c0-d9bb020a4f45",
   "metadata": {},
   "source": [
    "### 4- Delete the checkpoints to reclaim some storage space in the storage file system\n",
    "\n",
    "The default **checkpoint garbage collection policy** dictates Determined to save the most recent and the best checkpoint per training task (trial). The ***save_experiment_best***, ***save_trial_best*** and ***save_trial_latest*** parameters specify which checkpoints to save. The default policy is set as follows:\n",
    "\n",
    "  * save_experiment_best:0 \n",
    "  * save_trial_best:1\n",
    "  * save_trial_latest:1\n",
    " \n",
    "#### Run the code cell below to reclaim some storage disk space by changing the default checkpoint garbage collection policy as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39e584-2caf-4fe1-bb8b-a72f1b88888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the checkpoints data for the distributed training\n",
    "!~/.local/bin/det -m {determined_master} experiment set gc-policy --yes --save-experiment-best 0 --save-trial-best 0 --save-trial-latest 0 {myexpId}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658f8a59-040e-489c-b114-96820feaf017",
   "metadata": {},
   "source": [
    "#### Now, logout from your local Jupyter Notebook and go back to the JupyterHub notebook to perform some cleanup and for a wrap up of the workshop."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
